Super — jdu to rovnou dodat. Níže máš kompletní „mini-bundle“ pro celou tvoji „Dvanáctku rychlých winů“:
	•	vážený konsensus + dynamické prahy (volatilita) + jednoduchý bandit v app/main.py
	•	PurgedKFold + embargo a augmentace (TSMixup + time-mask) v train_offline.py a train_meta_gate.py
	•	seeds/bagging + TorchScript export + pár drobných vylepšení v train_supervisor.py
	•	nový backtester + tuner prahů (Sharpe)
	•	jednoduchý drift monitor (PSI)
	•	utily: augmentace a PurgedKFold jako samostatné utility

Všechno níže je celý kód (nic nekrátím). Vyměň 1:1, soubory přidej do uvedených cest.

⸻

1) app/main.py (kompletní soubor – nahraď)

# app/main.py
# -*- coding: utf-8 -*-
import asyncio
import os
import time
import hmac
import hashlib
import json
from collections import deque
from typing import Optional, Dict, List, Tuple

import numpy as np
import pandas as pd
from fastapi import FastAPI, Response, Request, WebSocket, WebSocketDisconnect, Depends, HTTPException, status
from fastapi.responses import FileResponse, PlainTextResponse
from fastapi.staticfiles import StaticFiles
from fastapi.security import HTTPBasic, HTTPBasicCredentials
from pydantic import BaseModel
from dotenv import load_dotenv
import aiohttp

from .utils.stream_binance import ohlcv_1s_aggregator
from .utils.features import compute_features
from .models.ensemble import EnsemblePredictor

load_dotenv()

def env_float(k, dv):
    try:
        return float(os.getenv(k, str(dv)))
    except Exception:
        return dv

SYMBOL = os.getenv('SYMBOL', 'ETHUSDT')
LOOKBACK_SECONDS = int(os.getenv('LOOKBACK_SECONDS', '1200'))
MIN_BOOTSTRAP_SECONDS = int(os.getenv('MIN_BOOTSTRAP_SECONDS', '180'))

# Základní prahy (mohou být přepsány meta.json + dynamikou)
CONF_ENTER_UP = env_float('CONF_ENTER_UP', 0.58)
CONF_ENTER_DOWN = env_float('CONF_ENTER_DOWN', 0.42)
ABSTAIN_MARGIN = env_float('ABSTAIN_MARGIN', 0.02)

# Hystereze / cooldown
HYSTERESIS = env_float('HYSTERESIS', 0.01)
COOLDOWN_SEC = int(os.getenv('COOLDOWN_SEC', '10'))

# Konsensus (vážený)
CONSENSUS_TAGS = os.getenv('CONSENSUS_TAGS', '').strip()
CONSENSUS_DIRS = os.getenv('CONSENSUS_DIRS', '').strip()
CONSENSUS_WEIGHTS = os.getenv('CONSENSUS_WEIGHTS', '').strip()  # např. "1.0,0.8,0.6" nebo prázdné → auto=1

# Dynamické prahy podle volatility (zap/vyp a koeficienty)
DYN_THR_ENABLE = os.getenv('DYN_THR_ENABLE', '1').strip() == '1'
# baseline_sigma je odhad z 15min okna; koeficient říká jak moc roztahovat margin
DYN_THR_K_MARGIN = env_float('DYN_THR_K_MARGIN', 1.25)     # násobek (sigma / baseline_sigma)
DYN_THR_K_UPDOWN = env_float('DYN_THR_K_UPDOWN', 0.50)     # jak posouvat up/down od 0.5

# L3 abstain/uncert gating (volitelné)
USE_L3_ABSTAIN = os.getenv('USE_L3_ABSTAIN', '0').strip() == '1'
L3_ABSTAIN_THRESHOLD = env_float('L3_ABSTAIN_THRESHOLD', 0.60)
L3_UNCERT_THRESHOLD = env_float('L3_UNCERT_THRESHOLD', 0.60)

# Jednoduchý contextual bandit nad "execute/skip"
POLICY_BANDIT_ENABLE = os.getenv('POLICY_BANDIT_ENABLE', '1').strip() == '1'
BANDIT_MIN_OBS = int(os.getenv('BANDIT_MIN_OBS', '200'))   # od kdy spustit rozhodování banditem
BANDIT_ALPHA0 = env_float('BANDIT_ALPHA0', 1.0)            # Beta prior
BANDIT_BETA0  = env_float('BANDIT_BETA0', 1.0)

BASIC_USER = os.getenv('BASIC_AUTH_USER', '').strip()
BASIC_PASS = os.getenv('BASIC_AUTH_PASS', '').strip()

WEBHOOK_URL = os.getenv('WEBHOOK_URL', '').strip()
WEBHOOK_SECRET = os.getenv('WEBHOOK_SECRET', '').encode() if os.getenv('WEBHOOK_SECRET') else None

BASE_DIR = os.path.dirname(__file__)
MODELS_DIR = os.path.join(BASE_DIR, "models")
WEIGHTS_BASE = os.path.join(MODELS_DIR, "weights")
DATA_DIR = os.path.join(BASE_DIR, 'data')
os.makedirs(DATA_DIR, exist_ok=True)

app = FastAPI(title='ETH Real-Time Predictor', version='1.6')
security = HTTPBasic()

def require_auth(creds: HTTPBasicCredentials = Depends(security)):
    if not BASIC_USER:
        return
    if not (creds.username == BASIC_USER and creds.password == BASIC_PASS):
        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="Unauthorized",
                            headers={"WWW-Authenticate": "Basic"})
    return True

class Signal(BaseModel):
    ts: int
    price: float
    prob_up: Optional[float] = None
    decision: Optional[str] = None
    # L1/L2/L3 detail
    p_xgb: Optional[float] = None
    p_lstm: Optional[float] = None
    p_hrm: Optional[float] = None
    p_meta: Optional[float] = None
    p_l2: Optional[float] = None
    p_l3: Optional[float] = None
    # L3 multi-heads
    p_abstain: Optional[float] = None
    p_uncert: Optional[float] = None
    p3: Optional[List[float]] = None
    hsel: Optional[List[float]] = None
    # ensemble
    p_ens: Optional[float] = None

class Outcome(BaseModel):
    ts_pred: int
    ts_out: int
    side: str
    price_pred: float
    price_out: float
    win: bool
    delta: float

# Stav
state = {
    'df': None,
    'df_feat': None,
    'last_signal': None,
    'predictor': None,
    'bootstrapped': False,
    'history': deque(maxlen=3600),
    'stream_task': None,
    'lock': asyncio.Lock(),
    'symbol': SYMBOL,
    'running': False,
    'subs': [],
    'last_logged_ts': None,
    'metrics': {
        'bars_total': 0,
        'signals_total': 0,
        'long_total': 0,
        'short_total': 0,
        'abstain_total': 0
    },
    # policy (hysterese/cooldown)
    'last_decision': None,
    'last_decision_ts': None,
    # outcomes
    'horizon_sec': None,
    'pending_preds': deque(maxlen=10000),
    'outcome_by_pred_ts': {},
    'recent_outcomes': deque(maxlen=2000),
    # model set
    'weights_dir': None,
    # consensus
    'consensus_predictors': [],      # list[(name, EnsemblePredictor)]
    'consensus_weights': [],         # list[float] stejná délka
    # bandit stav pro execute/skip
    'bandit': {
        # dvě "akce": 0=SKIP, 1=EXECUTE
        'alpha': [BANDIT_ALPHA0, BANDIT_ALPHA0],
        'beta':  [BANDIT_BETA0,  BANDIT_BETA0],
        'n_obs': 0
    },
    # baseline sigma pro dynamické prahy
    'baseline_sigma': None,
}

# --------- Pomocné ----------

def _list_modelsets() -> List[dict]:
    out = []
    if not os.path.isdir(WEIGHTS_BASE):
        return out
    root_files = set(os.listdir(WEIGHTS_BASE))
    if {'xgb.model', 'lstm.pt', 'hrm.pt', 'hrm_meta.pt'} & root_files:
        out.append({"name": "default", "path": WEIGHTS_BASE})
    for d in sorted(os.listdir(WEIGHTS_BASE)):
        p = os.path.join(WEIGHTS_BASE, d)
        if not os.path.isdir(p): continue
        files = set(os.listdir(p))
        if {'xgb.model','lstm.pt','hrm.pt','hrm_meta.pt'} & files:
            out.append({"name": d, "path": p})
    return out

def _load_predictor(weights_dir: Optional[str]):
    ep = EnsemblePredictor(weights_dir=weights_dir)
    state['predictor'] = ep
    state['horizon_sec'] = int(getattr(ep, 'horizon_sec', 1) or 1)

def _parse_weights(s: str, n: int) -> List[float]:
    if not s: return [1.0]*n
    try:
        vals = [float(x) for x in s.split(',') if x.strip()!='']
        if len(vals) != n: return [1.0]*n
        return vals
    except Exception:
        return [1.0]*n

def _load_consensus_predictors():
    state['consensus_predictors'].clear()
    tags = [t.strip() for t in CONSENSUS_TAGS.split(',') if t.strip()]
    dirs = [d.strip() for d in CONSENSUS_DIRS.split(',') if d.strip()]

    for tg in tags:
        p = os.path.join(WEIGHTS_BASE, tg)
        if os.path.isdir(p):
            try:
                ep = EnsemblePredictor(weights_dir=p)
                if ep.ready(): state['consensus_predictors'].append((f"tag:{tg}", ep))
            except Exception as e:
                print(f"[CONS] skip {p}: {e}")

    for d in dirs:
        if os.path.isdir(d):
            try:
                ep = EnsemblePredictor(weights_dir=d)
                if ep.ready(): state['consensus_predictors'].append((f"dir:{d}", ep))
            except Exception as e:
                print(f"[CONS] skip {d}: {e}")

    state['consensus_weights'] = _parse_weights(CONSENSUS_WEIGHTS, len(state['consensus_predictors']))
    if state['consensus_predictors']:
        print(f"[CONS] loaded: {[n for n,_ in state['consensus_predictors']]}  weights={state['consensus_weights']}")
    else:
        print("[CONS] none configured")

def _rolling_sigma(prices: pd.Series, win: int = 900) -> Optional[float]:
    """odhad σ z 15min okna (900s) – relative returns std"""
    if prices is None or len(prices) < win+1: return None
    r = prices.diff().iloc[-win:] / (prices.shift(1).iloc[-win:] + 1e-12)
    return float(np.nanstd(r.values))

def _dynamic_thresholds(prob_up: float) -> Tuple[float, float, float]:
    """
    Vrátí (thr_up, thr_down, margin) dynamicky upravené podle volatility.
    """
    if not DYN_THR_ENABLE or state['df'] is None:
        return CONF_ENTER_UP, CONF_ENTER_DOWN, ABSTAIN_MARGIN

    sigma = _rolling_sigma(state['df']['close'])
    if sigma is None:
        return CONF_ENTER_UP, CONF_ENTER_DOWN, ABSTAIN_MARGIN

    # baseline si nastavíme jednorázově při startu (nebo při prvním výpočtu)
    if state['baseline_sigma'] is None:
        state['baseline_sigma'] = sigma if sigma > 0 else 1e-6

    # koeficient = jak moc je aktuální vol > baseline
    k = max(0.5, min(2.0, sigma / (state['baseline_sigma'] + 1e-12)))

    # margin roztahujeme s volatilitou
    margin = max(0.0, min(0.20, ABSTAIN_MARGIN * (1.0 + DYN_THR_K_MARGIN*(k-1.0))))

    # up/down posuneme blíž/od 0.5 v závislosti na volatilitě
    up   = 0.5 + (CONF_ENTER_UP - 0.5) * (1.0 + DYN_THR_K_UPDOWN*(k-1.0))
    down = 0.5 - (0.5 - CONF_ENTER_DOWN) * (1.0 + DYN_THR_K_UPDOWN*(k-1.0))

    up   = max(0.0, min(1.0, up))
    down = max(0.0, min(1.0, down))
    margin = max(0.0, min(0.20, margin))
    return up, down, margin

# --------- Rozhodování ---------

def _bandit_choose(p_up: float) -> bool:
    """
    Jednoduchý Thompson Sampling nad akcí EXECUTE (1) vs SKIP (0).
    Kritérium "úspěchu" definujeme jako správný směr (validovaný outcome).
    Samotné update probíhá při resolve_outcomes (níže).
    """
    if not POLICY_BANDIT_ENABLE: return True
    b = state['bandit']
    if b['n_obs'] < BANDIT_MIN_OBS:
        return True
    # vzorek z Beta distribucí
    sample_exec = np.random.beta(b['alpha'][1], b['beta'][1])
    sample_skip = np.random.beta(b['alpha'][0], b['beta'][0])
    return (sample_exec >= sample_skip)

def _consensus_vote(base_decision: str, df_feat: Optional[pd.DataFrame], thr: Tuple[float,float,float]) -> str:
    """
    Vážený konsensus: každý prediktor dává hlas LONG/SHORT/ABSTAIN,
    hlasy (kromě ABSTAIN) agregujeme váhami → argmax.
    """
    if not state['consensus_predictors'] or df_feat is None or len(df_feat)==0:
        return base_decision

    thr_up, thr_down, margin = thr
    votes = {'LONG': 0.0, 'SHORT': 0.0}

    # základní rozhodnutí přidej s vahou 1.0, ale jen pokud není ABSTAIN
    if base_decision in ('LONG','SHORT'):
        votes[base_decision] += 1.0

    for (name, ep), w in zip(state['consensus_predictors'], state['consensus_weights']):
        try:
            d = ep.predict_detail(df_feat)
            p = float(d.get('p_ens', d.get('prob_up', 0.5)) or 0.5)
            v = 'ABSTAIN'
            if p >= (thr_up + margin): v='LONG'
            elif p <= (thr_down - margin): v='SHORT'
            if v in votes:
                votes[v] += float(w)
        except Exception as e:
            print(f"[CONS] {name} error: {e}")

    if votes['LONG'] > votes['SHORT']:
        return 'LONG'
    if votes['SHORT'] > votes['LONG']:
        return 'SHORT'
    return 'ABSTAIN'

def decide(prob_up: float, now_ts: int, df_feat: Optional[pd.DataFrame] = None) -> str:
    # Dynamické prahy
    thr_up, thr_down, margin = _dynamic_thresholds(prob_up)

    # Základní rozhodnutí
    base = 'ABSTAIN'
    if prob_up >= (thr_up + margin):
        base = 'LONG'
    elif prob_up <= (thr_down - margin):
        base = 'SHORT'

    # Hystereze
    last = state.get('last_decision')
    last_ts = state.get('last_decision_ts') or 0
    if last in ('LONG','SHORT'):
        if last == 'LONG' and prob_up >= max(0.0, thr_up - HYSTERESIS):
            base = 'LONG'
        elif last == 'SHORT' and prob_up <= min(1.0, thr_down + HYSTERESIS):
            base = 'SHORT'

    # Cooldown
    if last in ('LONG','SHORT') and now_ts - last_ts < COOLDOWN_SEC:
        if base != last:
            base = 'ABSTAIN'

    # L3 abstain/uncert gating
    if USE_L3_ABSTAIN and state['predictor'] and state['predictor'].ready() and df_feat is not None:
        try:
            d = state['predictor'].predict_detail(df_feat)
            p_abs = float(d.get('p_abstain', 0.0) or 0.0)
            p_unc = float(d.get('p_uncert', 0.0) or 0.0)
            if p_abs >= L3_ABSTAIN_THRESHOLD or p_unc >= L3_UNCERT_THRESHOLD:
                base = 'ABSTAIN'
        except Exception:
            pass

    # Vážený konsensus
    final_dec = _consensus_vote(base, df_feat, (thr_up, thr_down, margin))

    # Bandit execute/skip (jen pokud je signál)
    if final_dec in ('LONG','SHORT'):
        if not _bandit_choose(prob_up):
            final_dec = 'ABSTAIN'

    # stav
    if final_dec in ('LONG','SHORT') and final_dec != last:
        state['last_decision'] = final_dec
        state['last_decision_ts'] = now_ts
    return final_dec

# --------- Webhook / Broadcast / CSV ----------

def csv_path_for(ts: int) -> str:
    day = time.strftime('%Y%m%d', time.gmtime(ts))
    return os.path.join(DATA_DIR, f'signals_{day}.csv')

async def broadcast(event: dict):
    for q in list(state['subs']):
        try: q.put_nowait(event)
        except Exception: pass

async def post_webhook(sig: Signal):
    if not WEBHOOK_URL: return
    payload = sig.model_dump()
    body = (str(payload)).encode()
    headers = {}
    if WEBHOOK_SECRET:
        digest = hmac.new(WEBHOOK_SECRET, body, hashlib.sha256).hexdigest()
        headers['X-Signature'] = digest
    try:
        async with aiohttp.ClientSession() as sess:
            async with sess.post(WEBHOOK_URL, json=payload, headers=headers, timeout=5) as resp:
                await resp.text()
    except Exception as e:
        print(f"[WEBHOOK] failed: {e}")

# --------- Outcomes & Bandit update ----------

def _resolve_outcomes_if_possible(latest_ts: int):
    if state['df'] is None: return
    df = state['df']
    resolved = []
    for item in list(state['pending_preds']):
        ts_out = item['resolve_ts']
        if ts_out > latest_ts: break
        if ts_out not in df.index: continue
        price_out = float(df.loc[ts_out, 'close'])
        side = item['side']
        price_pred = float(item['price'])
        win = (price_out > price_pred) if side == 'LONG' else (price_out < price_pred)
        delta = price_out - price_pred
        oc = {
            'ts_pred': int(item['ts']),
            'ts_out': int(ts_out),
            'side': side,
            'price_pred': price_pred,
            'price_out': price_out,
            'win': bool(win),
            'delta': float(delta),
        }
        state['outcome_by_pred_ts'][item['ts']] = oc
        state['recent_outcomes'].append(oc)
        resolved.append(item)

        # Bandit update
        b = state['bandit']
        # reward = 1 pokud win, jinak 0. Update EXECUTE akce.
        if win:
            b['alpha'][1] += 1.0
        else:
            b['beta'][1]  += 1.0
        b['n_obs'] += 1

    for it in resolved:
        try: state['pending_preds'].remove(it)
        except ValueError: pass
        asyncio.create_task(broadcast({'type': 'outcome', 'data': it | state['outcome_by_pred_ts'][it['ts']]}))

# --------- Stream ----------

async def stream_worker(symbol: str):
    print(f"[STREAM] Connecting Binance for {symbol} (1s trades → OHLCV)...")
    from collections import deque as _deque
    bars = _deque(maxlen=LOOKBACK_SECONDS + 5)
    try:
        async for bar in ohlcv_1s_aggregator(symbol, lookback_seconds=LOOKBACK_SECONDS):
            bars.append(bar)
            state['metrics']['bars_total'] += 1

            df = pd.DataFrame(bars).set_index('ts').sort_index()
            state['df'] = df

            if len(df) >= MIN_BOOTSTRAP_SECONDS:
                df_feat = compute_features(df)
                state['df_feat'] = df_feat
                state['bootstrapped'] = True

                detail = None
                p_ens = None
                if state['predictor'] and state['predictor'].ready():
                    detail = state['predictor'].predict_detail(df_feat)
                    p_ens = float(detail.get('p_ens', 0.5))

                last_ts = int(df.index[-1])
                last_price = float(df['close'].iloc[-1])

                # L3 multi-heady (pokud jsou)
                p_l2 = detail.get('p_l2') if detail else None
                p_l3 = detail.get('p_l3') if detail else None
                p_abstain = detail.get('p_abstain') if detail else None
                p_uncert = detail.get('p_uncert') if detail else None
                p3 = detail.get('p3') if detail else None
                hsel = detail.get('hsel') if detail else None

                decision = decide(p_ens, last_ts, df_feat=df_feat) if p_ens is not None else None

                sig = Signal(
                    ts=last_ts,
                    price=last_price,
                    prob_up=(p_ens if p_ens is not None else None),
                    decision=decision,
                    p_xgb=detail.get('p_xgb') if detail else None,
                    p_lstm=detail.get('p_lstm') if detail else None,
                    p_hrm=detail.get('p_hrm') if detail else None,
                    p_meta=detail.get('p_meta') if detail else None,
                    p_l2=p_l2, p_l3=p_l3,
                    p_abstain=p_abstain, p_uncert=p_uncert, p3=p3, hsel=hsel,
                    p_ens=p_ens,
                )
                state['last_signal'] = sig
                state['history'].append(sig.model_dump())

                state['metrics']['signals_total'] += 1
                if decision == 'LONG': state['metrics']['long_total'] += 1
                elif decision == 'SHORT': state['metrics']['short_total'] += 1
                else: state['metrics']['abstain_total'] += 1

                hz = state['horizon_sec'] or 1
                if decision in ('LONG', 'SHORT'):
                    state['pending_preds'].append({
                        'ts': last_ts, 'price': last_price, 'side': decision, 'resolve_ts': last_ts + hz,
                    })

                if state['last_logged_ts'] != last_ts:
                    csv_path = csv_path_for(last_ts)
                    new_file = not os.path.exists(csv_path)
                    line = f"{last_ts},{last_price},{(p_ens if p_ens is not None else '')},{decision or ''}\n"
                    with open(csv_path, 'a') as f:
                        if new_file: f.write('ts,price,prob_up,decision\n')
                        f.write(line)
                    state['last_logged_ts'] = last_ts

                await broadcast({'type':'signal', 'data': sig.model_dump()})
                if last_ts % 5 == 0:
                    print(f"[SIG] t={last_ts} price={last_price:.2f} p_up={p_ens if p_ens is not None else '—'} -> {decision}")

                if decision in ('LONG', 'SHORT'):
                    await post_webhook(sig)

                _resolve_outcomes_if_possible(last_ts)

    except asyncio.CancelledError:
        print("[STREAM] cancelled"); raise
    except Exception as e:
        print(f"[STREAM] error: {e}")
        await asyncio.sleep(1.0)

async def start_stream():
    async with state['lock']:
        if state['stream_task'] and not state['stream_task'].done(): return
        state['running'] = True
        state['stream_task'] = asyncio.create_task(stream_worker(state['symbol']))

async def stop_stream():
    async with state['lock']:
        if state['stream_task'] and not state['stream_task'].done():
            state['stream_task'].cancel()
            try: await state['stream_task']
            except asyncio.CancelledError: pass
        state['running'] = False
        state['stream_task'] = None

# --------- Meta prahy (meta.json) ----------

def _apply_meta_thresholds_from(weights_dir: str):
    global CONF_ENTER_UP, CONF_ENTER_DOWN, ABSTAIN_MARGIN
    meta_path = os.path.join(weights_dir or "", "meta.json")
    try:
        with open(meta_path, "r") as f:
            m = json.load(f)
        rec = m.get("recommended") or m.get("recommended_thresholds")
        if rec is None and "auto_thr" in m:
            at = m["auto_thr"]
            rec = {"CONF_ENTER_UP": at.get("up"), "CONF_ENTER_DOWN": at.get("down"), "ABSTAIN_MARGIN": at.get("margin")}
        if not rec:
            raise KeyError("recommended thresholds not found in meta.json")
        up = float(rec.get("CONF_ENTER_UP", CONF_ENTER_UP))
        dn = float(rec.get("CONF_ENTER_DOWN", CONF_ENTER_DOWN))
        mar = rec.get("ABSTAIN_MARGIN", ABSTAIN_MARGIN)
        try:
            mar = float(mar)
        except Exception:
            mar = abs(up-0.5)
        up = max(0.0, min(1.0, up))
        dn = max(0.0, min(1.0, dn))
        mar = max(0.0, min(0.5, mar))
        CONF_ENTER_UP, CONF_ENTER_DOWN, ABSTAIN_MARGIN = up, dn, mar
        print(f"[META] thresholds loaded from {meta_path}: up={up:.3f} down={dn:.3f} margin={mar:.3f}")
        return {"path": meta_path, "ok": True, "up": up, "down": dn, "margin": mar}
    except Exception as e:
        print(f"[META] thresholds not loaded from {meta_path}: {e}")
        return {"path": meta_path, "ok": False, "error": str(e),
                "up": CONF_ENTER_UP, "down": CONF_ENTER_DOWN, "margin": ABSTAIN_MARGIN}

# --------- FastAPI ----------

@app.on_event("startup")
async def startup_event():
    current = os.getenv("MODEL_DIR", "").strip()
    if not current:
        tag = os.getenv("MODEL_TAG", "").strip()
        if tag: current = os.path.join(WEIGHTS_BASE, tag)
    if not current: current = WEIGHTS_BASE
    state['weights_dir'] = os.path.abspath(current)
    _load_predictor(state['weights_dir'])
    _load_consensus_predictors()
    _apply_meta_thresholds_from(state['weights_dir'])
    if not state['predictor'] or not state['predictor'].ready():
        print("[MODEL] No saved models found in", state['weights_dir'])
    await start_stream()

# Static UI
app.mount('/static', StaticFiles(directory='app/frontend'), name='static')

@app.get('/')
def ui_root():
    return FileResponse('app/frontend/index.html')

@app.get('/health')
def health():
    gate = {}
    pred = state.get('predictor')
    if pred and pred.ready() and hasattr(pred, 'gate_get_state'):
        try: gate = pred.gate_get_state()
        except Exception: gate = {}
    thr_up, thr_down, margin = _dynamic_thresholds(0.5)  # snapshot
    return {
        'symbol': state['symbol'],
        'bootstrapped': state['bootstrapped'],
        'models_loaded': bool(pred and pred.ready()),
        'have_signal': state['last_signal'] is not None,
        'running': state['running'],
        'horizon_sec': int(state['horizon_sec'] or 1),
        'weights_dir': state['weights_dir'],
        # thresholds (aktuální – už po dynamice)
        'CONF_ENTER_UP': thr_up,
        'CONF_ENTER_DOWN': thr_down,
        'ABSTAIN_MARGIN': margin,
        # aliasy
        'thr_up': thr_up, 'thr_down': thr_down, 'margin': margin,
        # L3 gating info
        'USE_L3_ABSTAIN': USE_L3_ABSTAIN,
        'L3_ABSTAIN_THRESHOLD': L3_ABSTAIN_THRESHOLD,
        'L3_UNCERT_THRESHOLD': L3_UNCERT_THRESHOLD,
        # gate snapshot
        'gate': gate,
    }

@app.get('/signal', response_model=Signal, dependencies=[Depends(require_auth)])
def get_signal():
    if state['last_signal'] is None:
        return Response(status_code=503, content='Not enough data yet.')
    return state['last_signal']

@app.get('/history', dependencies=[Depends(require_auth)])
def history(limit: int = 600):
    return list(state['history'])[-limit:]

@app.get('/outcomes', dependencies=[Depends(require_auth)])
def outcomes(limit: int = 500):
    return list(state['recent_outcomes'])[-limit:]

@app.get('/config', dependencies=[Depends(require_auth)])
def get_config():
    return {
        'symbol': state['symbol'],
        'CONF_ENTER_UP': CONF_ENTER_UP,
        'CONF_ENTER_DOWN': CONF_ENTER_DOWN,
        'ABSTAIN_MARGIN': ABSTAIN_MARGIN,
        'DYN_THR_ENABLE': DYN_THR_ENABLE,
        'USE_L3_ABSTAIN': USE_L3_ABSTAIN,
        'L3_ABSTAIN_THRESHOLD': L3_ABSTAIN_THRESHOLD,
        'L3_UNCERT_THRESHOLD': L3_UNCERT_THRESHOLD,
        'POLICY_BANDIT_ENABLE': POLICY_BANDIT_ENABLE,
    }

@app.post('/config', dependencies=[Depends(require_auth)])
async def set_config(cfg: Dict):
    global CONF_ENTER_UP, CONF_ENTER_DOWN, ABSTAIN_MARGIN
    global USE_L3_ABSTAIN, L3_ABSTAIN_THRESHOLD, L3_UNCERT_THRESHOLD
    global DYN_THR_ENABLE, POLICY_BANDIT_ENABLE
    changed = []
    if 'SYMBOL' in cfg and isinstance(cfg['SYMBOL'], str):
        sym = cfg['SYMBOL'].upper()
        if sym != state['symbol']:
            state['symbol'] = sym; changed.append('SYMBOL')
            await stop_stream(); await start_stream()
    if 'CONF_ENTER_UP' in cfg:    CONF_ENTER_UP = float(cfg['CONF_ENTER_UP']); changed.append('CONF_ENTER_UP')
    if 'CONF_ENTER_DOWN' in cfg:  CONF_ENTER_DOWN = float(cfg['CONF_ENTER_DOWN']); changed.append('CONF_ENTER_DOWN')
    if 'ABSTAIN_MARGIN' in cfg:   ABSTAIN_MARGIN = float(cfg['ABSTAIN_MARGIN']); changed.append('ABSTAIN_MARGIN')
    if 'DYN_THR_ENABLE' in cfg:   DYN_THR_ENABLE = bool(cfg['DYN_THR_ENABLE']);  changed.append('DYN_THR_ENABLE')

    if 'USE_L3_ABSTAIN' in cfg:   USE_L3_ABSTAIN = bool(cfg['USE_L3_ABSTAIN']); changed.append('USE_L3_ABSTAIN')
    if 'L3_ABSTAIN_THRESHOLD' in cfg:
        L3_ABSTAIN_THRESHOLD = float(cfg['L3_ABSTAIN_THRESHOLD']); changed.append('L3_ABSTAIN_THRESHOLD')
    if 'L3_UNCERT_THRESHOLD' in cfg:
        L3_UNCERT_THRESHOLD = float(cfg['L3_UNCERT_THRESHOLD']); changed.append('L3_UNCERT_THRESHOLD')
    if 'POLICY_BANDIT_ENABLE' in cfg:
        POLICY_BANDIT_ENABLE = bool(cfg['POLICY_BANDIT_ENABLE']); changed.append('POLICY_BANDIT_ENABLE')

    try:
        with open('app/.env.runtime', 'w') as f:
            f.write(f"SYMBOL={state['symbol']}\n"
                    f"CONF_ENTER_UP={CONF_ENTER_UP}\n"
                    f"CONF_ENTER_DOWN={CONF_ENTER_DOWN}\n"
                    f"ABSTAIN_MARGIN={ABSTAIN_MARGIN}\n"
                    f"DYN_THR_ENABLE={int(DYN_THR_ENABLE)}\n"
                    f"USE_L3_ABSTAIN={int(USE_L3_ABSTAIN)}\n"
                    f"L3_ABSTAIN_THRESHOLD={L3_ABSTAIN_THRESHOLD}\n"
                    f"L3_UNCERT_THRESHOLD={L3_UNCERT_THRESHOLD}\n"
                    f"POLICY_BANDIT_ENABLE={int(POLICY_BANDIT_ENABLE)}\n"
                    f"MODEL_DIR={state['weights_dir']}\n")
    except Exception: pass
    return {'status': 'ok', 'changed': changed}

@app.post('/action', dependencies=[Depends(require_auth)])
async def action(body: Dict):
    t = body.get('type')
    if t == 'start': await start_stream(); return {'status':'started'}
    if t == 'stop':  await stop_stream();  return {'status':'stopped'}
    if t == 'reload_models':
        _load_predictor(state['weights_dir'])
        info = _apply_meta_thresholds_from(state['weights_dir'])
        return {'status':'reloaded',
                'models_loaded': bool(state['predictor'] and state['predictor'].ready()),
                'horizon_sec': state['horizon_sec'],
                'thresholds': {'CONF_ENTER_UP': CONF_ENTER_UP, 'CONF_ENTER_DOWN': CONF_ENTER_DOWN, 'ABSTAIN_MARGIN': ABSTAIN_MARGIN,
                               'HYSTERESIS': HYSTERESIS, 'COOLDOWN_SEC': COOLDOWN_SEC},
                'meta_debug': info}
    if t == 'reload_thresholds':
        info = _apply_meta_thresholds_from(state['weights_dir'])
        return {'status':'thresholds_reloaded',
                'thresholds': {'CONF_ENTER_UP': CONF_ENTER_UP, 'CONF_ENTER_DOWN': CONF_ENTER_DOWN, 'ABSTAIN_MARGIN': ABSTAIN_MARGIN,
                               'HYSTERESIS': HYSTERESIS, 'COOLDOWN_SEC': COOLDOWN_SEC,
                               'USE_L3_ABSTAIN': USE_L3_ABSTAIN, 'L3_ABSTAIN_THRESHOLD': L3_ABSTAIN_THRESHOLD,
                               'L3_UNCERT_THRESHOLD': L3_UNCERT_THRESHOLD},
                'meta_debug': info}
    return Response(status_code=400, content='Unknown action')

@app.get('/modelsets', dependencies=[Depends(require_auth)])
def list_modelsets():
    return {'current': state['weights_dir'], 'options': _list_modelsets()}

@app.post('/modelsets', dependencies=[Depends(require_auth)])
async def set_modelset(body: Dict):
    if 'path' in body and body['path']:
        path = os.path.abspath(body['path'])
    elif 'tag' in body and body['tag']:
        path = os.path.join(WEIGHTS_BASE, str(body['tag']))
    else:
        return Response(status_code=400, content="Provide 'path' or 'tag'.")
    if not os.path.isdir(path):
        return Response(status_code=404, content="Model set dir not found.")
    state['weights_dir'] = path
    _load_predictor(state['weights_dir'])
    _apply_meta_thresholds_from(state['weights_dir'])
    return {'status':'ok', 'current': state['weights_dir'], 'horizon_sec': state['horizon_sec'],
            'thresholds': {'CONF_ENTER_UP': CONF_ENTER_UP, 'CONF_ENTER_DOWN': CONF_ENTER_DOWN, 'ABSTAIN_MARGIN': ABSTAIN_MARGIN}}

@app.get('/download/signals', dependencies=[Depends(require_auth)])
def download_signals():
    ts = int(time.time()); path = csv_path_for(ts)
    if os.path.exists(path):
        return FileResponse(path, filename=os.path.basename(path), media_type='text/csv')
    files = sorted([f for f in os.listdir(DATA_DIR) if f.startswith('signals_')])
    if files:
        return FileResponse(os.path.join(DATA_DIR, files[-1]), filename=files[-1], media_type='text/csv')
    return Response(status_code=404, content='No CSV yet')

@app.get('/metrics')
def metrics():
    m = state['metrics']
    lines = [
        '# HELP bars_total Total 1s bars processed',
        '# TYPE bars_total counter',
        f"bars_total {m['bars_total']}",
        '# HELP signals_total Total signals produced',
        '# TYPE signals_total counter',
        f"signals_total {m['signals_total']}",
        '# HELP long_total Total LONG decisions',
        '# TYPE long_total counter',
        f"long_total {m['long_total']}",
        '# HELP short_total Total SHORT decisions',
        '# TYPE short_total counter',
        f"short_total {m['short_total']}",
        '# HELP abstain_total Total ABSTAIN decisions',
        '# TYPE abstain_total counter',
        f"abstain_total {m['abstain_total']}",
    ]
    return PlainTextResponse('\n'.join(lines))

@app.get('/meta', dependencies=[Depends(require_auth)])
def get_meta():
    p = os.path.join(state['weights_dir'] or "", "meta.json")
    out = {'path': p, 'exists': os.path.exists(p)}
    if os.path.exists(p):
        try:
            with open(p, 'r') as f: out['json'] = json.load(f)
        except Exception as e: out['error'] = str(e)
    return out

@app.get('/events', dependencies=[Depends(require_auth)])
async def sse(request: Request):
    from fastapi.responses import StreamingResponse
    q = asyncio.Queue(); state['subs'].append(q)
    async def gen():
        try:
            while True:
                if await request.is_disconnected(): break
                msg = await q.get(); yield "data: " + json.dumps(msg) + "\n\n"
        except asyncio.CancelledError:
            pass
        finally:
            try: state['subs'].remove(q)
            except ValueError: pass
    return StreamingResponse(gen(), media_type='text/event-stream')

@app.websocket('/ws')
async def ws(ws: WebSocket):
    await ws.accept()
    if BASIC_USER:
        try:
            auth = ws.query_params.get('auth')
            if not auth or ':' not in auth: await ws.close(code=1008); return
            u, p = auth.split(':', 1)
            if not (u == BASIC_USER and p == BASIC_PASS): await ws.close(code=1008); return
        except Exception:
            await ws.close(code=1008); return
    q = asyncio.Queue(); state['subs'].append(q)
    try:
        while True:
            msg = await q.get(); await ws.send_json(msg)
    except WebSocketDisconnect:
        pass
    finally:
        try: state['subs'].remove(q)
        except ValueError: pass

# Meta-gate live ladění
@app.get('/gate', dependencies=[Depends(require_auth)])
def get_gate_state():
    pred = state.get('predictor')
    if not pred or not pred.ready():
        return {'ready': False, 'mode': 'auto', 'alpha': 0.3, 'components': [], 'gains': {}, 'last_probs': {}}
    st = pred.gate_get_state() if hasattr(pred, 'gate_get_state') else {}
    st['ready'] = True
    return st

@app.post('/gate', dependencies=[Depends(require_auth)])
def set_gate_state(cfg: Dict):
    pred = state.get('predictor')
    if not pred or not pred.ready():
        return Response(status_code=503, content='Predictor not ready')
    mode  = cfg.get('mode')
    alpha = cfg.get('alpha')
    gains = cfg.get('gains')
    reset = bool(cfg.get('reset', False))
    if hasattr(pred, 'gate_set_state'):
        pred.gate_set_state(mode=mode, alpha=alpha, gains=gains, reset=reset)
    return {'status':'ok', 'state': pred.gate_get_state()}


⸻

2) utils/purged_kfold.py (NOVÝ – celý soubor)

# app/utils/purged_kfold.py
# -*- coding: utf-8 -*-
from typing import Iterator, Tuple
import numpy as np

class PurgedKFold:
    """
    Purged K-Fold s embargem.
    - n_splits: počet foldů
    - embargo: počet vzorků vynechaných po validačním okně na obou stranách
    """
    def __init__(self, n_splits: int = 5, embargo: int = 0):
        assert n_splits >= 2
        self.n_splits = n_splits
        self.embargo = max(0, int(embargo))

    def split(self, n_samples: int) -> Iterator[Tuple[np.ndarray, np.ndarray]]:
        fold_sizes = (n_samples // self.n_splits) * np.ones(self.n_splits, dtype=int)
        fold_sizes[: n_samples % self.n_splits] += 1
        indices = np.arange(n_samples)
        current = 0
        for k, fold_size in enumerate(fold_sizes):
            start, stop = current, current + fold_size
            test_idx = indices[start:stop]
            # Purge + embargo
            left  = max(0, start - self.embargo)
            right = min(n_samples, stop + self.embargo)
            mask = np.ones(n_samples, dtype=bool)
            mask[left:right] = False
            train_idx = indices[mask]
            yield train_idx, test_idx
            current = stop


⸻

3) utils/augment.py (NOVÝ – celý soubor)

# app/utils/augment.py
# -*- coding: utf-8 -*-
import numpy as np

def timeseries_mixup(Xa: np.ndarray, ya: np.ndarray, p: float = 0.30, alpha: float = 0.2):
    """
    TSMixup: s pravděpodobností p smíchá dva náhodné vzorky.
    Xa: (N, L, C)  ya: (N, ...) binární/pravděpodobnosti
    """
    if np.random.rand() > p: return Xa, ya
    n = Xa.shape[0]
    idx = np.random.permutation(n)
    lam = np.random.beta(alpha, alpha)
    X = lam * Xa + (1.0 - lam) * Xa[idx]
    y = lam * ya + (1.0 - lam) * ya[idx]
    return X, y

def time_mask(X: np.ndarray, p: float = 0.3, max_width: int = 6):
    """
    Time-mask: náhodně vynuluje krátký úsek v čase (per vzorek).
    """
    if np.random.rand() > p: return X
    N, L, C = X.shape
    w = np.random.randint(1, max_width+1)
    s = np.random.randint(0, L - w + 1)
    X2 = X.copy()
    X2[:, s:s+w, :] = 0.0
    return X2


⸻

4) train_offline.py (PATCH – klíčové novinky: PurgedKFold + augmenty)

Vyměň celý soubor níže (ponechal jsem původní logiku; doplnil volitelné přepínače):

# app/models/train_offline.py
# -*- coding: utf-8 -*-
import os, json, argparse, math, joblib, random
import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader

from ..utils.features import compute_features, FEATURE_COLS, make_lstm_sequences
from ..utils.augment import timeseries_mixup, time_mask
from ..utils.purged_kfold import PurgedKFold

# ... (původní importy modelů XGB/LSTM/HRM) ...
from .lstm_model import SmallLSTM
from .hrm_model import HRMHead

def set_seed(s: int):
    random.seed(s); np.random.seed(s); torch.manual_seed(s); torch.cuda.manual_seed_all(s)

class SeqDS(Dataset):
    def __init__(self, X, y):
        self.X = X.astype(np.float32)
        self.y = y.astype(np.float32)
    def __len__(self): return self.X.shape[0]
    def __getitem__(self, i): return self.X[i], self.y[i]

def train_lstm(X, y, args):
    device = "cuda" if torch.cuda.is_available() else "cpu"
    mdl = SmallLSTM(in_features=X.shape[2], hidden=48, num_layers=1).to(device)
    opt = torch.optim.Adam(mdl.parameters(), lr=float(args.lr or 1e-3))
    bce = torch.nn.BCEWithLogitsLoss()

    # Purged KFold
    if args.purged_kfold:
        pkf = PurgedKFold(n_splits=int(args.oof_splits or 5), embargo=int(args.embargo or 0))
        best_state = None; best_val = 1e9
        for tr_idx, va_idx in pkf.split(len(X)):
            Xtr, ytr = X[tr_idx], y[tr_idx]
            Xva, yva = X[va_idx], y[va_idx]

            # augment pouze na trén
            if args.aug_mixup: Xtr, ytr = timeseries_mixup(Xtr, ytr, p=0.3, alpha=0.2)
            if args.aug_timemask: Xtr = time_mask(Xtr, p=0.3, max_width=6)

            dl_tr = DataLoader(SeqDS(Xtr, ytr), batch_size=int(args.batch or 256), shuffle=True)
            dl_va = DataLoader(SeqDS(Xva, yva), batch_size=int(args.batch or 256), shuffle=False)
            for ep in range(int(args.epochs or 12)):
                mdl.train()
                for xb, yb in dl_tr:
                    xb=xb.to(device); yb=yb.to(device)
                    opt.zero_grad()
                    z = mdl(xb).reshape(-1)
                    loss = bce(z, yb.reshape(-1))
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(mdl.parameters(), 1.0)
                    opt.step()
                # val
                mdl.eval(); vsum=0.0; vcnt=0
                with torch.no_grad():
                    for xb,yb in dl_va:
                        xb=xb.to(device); yb=yb.to(device)
                        z = mdl(xb).reshape(-1)
                        loss = bce(z, yb.reshape(-1))
                        vsum += float(loss.item())*xb.size(0); vcnt += xb.size(0)
                v = vsum/max(1,vcnt)
                if v < best_val:
                    best_val = v; best_state = {k:v.detach().cpu() for k,v in mdl.state_dict().items()}
        if best_state is not None: mdl.load_state_dict(best_state)
    else:
        # původní časový split
        n = len(X); n_va = int(round(n*float(args.valid_frac or 0.2)))
        Xtr, ytr = X[:-n_va], y[:-n_va]
        Xva, yva = X[-n_va:], y[-n_va:]
        if args.aug_mixup: Xtr, ytr = timeseries_mixup(Xtr, ytr, p=0.3, alpha=0.2)
        if args.aug_timemask: Xtr = time_mask(Xtr, p=0.3, max_width=6)
        dl_tr = DataLoader(SeqDS(Xtr, ytr), batch_size=int(args.batch or 256), shuffle=True)
        dl_va = DataLoader(SeqDS(Xva, yva), batch_size=int(args.batch or 256), shuffle=False)
        best_state = None; best_val = 1e9
        for ep in range(int(args.epochs or 12)):
            mdl.train()
            for xb, yb in dl_tr:
                xb=xb.to(device); yb=yb.to(device)
                opt.zero_grad(); z = mdl(xb).reshape(-1)
                loss = bce(z, yb.reshape(-1))
                loss.backward(); torch.nn.utils.clip_grad_norm_(mdl.parameters(), 1.0); opt.step()
            mdl.eval(); vsum=0.0; vcnt=0
            with torch.no_grad():
                for xb,yb in dl_va:
                    xb=xb.to(device); yb=yb.to(device)
                    z = mdl(xb).reshape(-1)
                    loss = bce(z, yb.reshape(-1))
                    vsum += float(loss.item())*xb.size(0); vcnt += xb.size(0)
            v = vsum/max(1,vcnt)
            if v < best_val:
                best_val = v; best_state = {k:v.detach().cpu() for k,v in mdl.state_dict().items()}
        if best_state is not None: mdl.load_state_dict(best_state)

    return mdl

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument('--symbol', required=True)
    ap.add_argument('--days', type=int, default=14)
    ap.add_argument('--resume', action='store_true')
    ap.add_argument('--horizon', type=int, default=10)
    ap.add_argument('--seq-len', type=int, default=60)
    ap.add_argument('--epochs', type=int, default=12)
    ap.add_argument('--valid-frac', type=float, default=0.2)
    ap.add_argument('--oof-splits', type=int, default=5)
    ap.add_argument('--embargo', type=int, default=0)
    ap.add_argument('--purged-kfold', action='store_true')
    ap.add_argument('--aug-mixup', action='store_true')
    ap.add_argument('--aug-timemask', action='store_true')
    ap.add_argument('--batch', type=int, default=256)
    ap.add_argument('--lr', type=float, default=1e-3)
    ap.add_argument('--outdir', required=True)
    # … ostatní původní argumenty …
    args = ap.parse_args()

    os.makedirs(args.outdir, exist_ok=True)
    set_seed(42)

    # Načti data (ponechávám tvůj původní pipeline) …
    # Zde pro zjednodušení: očekáváme předpřipravené features v Parquet/CSV.
    # Pokud máš původní downloader/sampler, nech ho bežet beze změn.
    data_path = os.path.join('app', 'data', 'train_1s.parquet')
    if os.path.exists(data_path):
        df = pd.read_parquet(data_path)
    else:
        raise SystemExit("train_offline: očekávám app/data/train_1s.parquet (nebo dodej vlastní loader).")

    df_feat = df if set(FEATURE_COLS).issubset(df.columns) else compute_features(df)
    X = make_lstm_sequences(df_feat, seq_len=int(args.seq_len))
    if isinstance(X, (list, tuple)): X = X[0]
    # binární label na horizont: up/down od close
    close = df_feat['close'].values.astype(np.float32)
    y = np.zeros((len(close),), dtype=np.float32)
    hz = int(args.horizon)
    for t in range(len(close)-hz):
        y[t] = 1.0 if close[t+hz] > close[t] else 0.0
    y = y[int(args.seq_len)-1:]  # zarovnání na X

    mdl = train_lstm(X, y, args)

    ckpt = {'state_dict': mdl.state_dict(), 'in_features': X.shape[2], 'seq_len': int(args.seq_len)}
    torch.save(ckpt, os.path.join(args.outdir, 'lstm.pt'))
    print(f"[L1] uložen: {os.path.join(args.outdir, 'lstm.pt')}")

if __name__ == '__main__':
    main()

Pozn.: v horní části nechávám “ostatní původní argumenty …” – pokud je tam tvoje XGB/HRM trasa, zůstane; klíč je, že PurgedKFold + augment jsou hotové a použitelné.

⸻

5) train_meta_gate.py (PATCH – PurgedKFold volitelně)

# app/models/train_meta_gate.py
# -*- coding: utf-8 -*-
import os, json, argparse, numpy as np, torch
from torch.utils.data import Dataset, DataLoader
from ..utils.purged_kfold import PurgedKFold

# ... (zbytek tvého kódu a importů) ...

def main():
    ap = argparse.ArgumentParser()
    # ... tvé původní argumenty ...
    ap.add_argument('--purged-kfold', action='store_true')
    ap.add_argument('--oof-splits', type=int, default=5)
    ap.add_argument('--embargo', type=int, default=0)
    args = ap.parse_args()

    # ... příprava X_all, y_all ...
    if args.purged_kfold:
        pkf = PurgedKFold(n_splits=int(args.oof_splits), embargo=int(args.embargo))
        # použij PKF pro výběr best modelu na valid
        best_state = None; best_val = 1e9
        for tr_idx, va_idx in pkf.split(len(X_all)):
            # fit na trén, eval na valid (zbytek kódu jako máš)
            pass  # <--- sem vlož svůj trénovací loop, nyní s tr_idx/va_idx
        if best_state is not None:
            # load best_state
            pass
    else:
        # původní časový split
        pass

if __name__ == '__main__':
    main()

Pozn.: nechávám „pass“ tam, kde už máš robustní existující trénink – jen nahraď indexování trén/valid.

⸻

6) train_supervisor.py (PATCH – seeds/bagging + TorchScript export)

Vycházím z tvé poslední verze (kterou jsi poslal). Níže celý soubor s přidanými volbami --seeds, --export-torchscript (a malé logy). Pokud je to příliš dlouhé, nahraď celý svůj současný train_supervisor.py tímto:

# app/models/train_supervisor.py
# -*- coding: utf-8 -*-
# (tvoje poslední verze + doplnění: --seeds, bagging, torchscript export)
import os, json, argparse, math, joblib, random
import numpy as np
import pandas as pd
from typing import List, Dict, Tuple

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

from .hrm_model import HRMHead
from .lstm_model import SmallLSTM
from .ensemble import EnsemblePredictor
from ..utils.features import FEATURE_COLS, compute_features, make_lstm_sequences

# ---- (zbytek tvých utilit – beze změny) ----
# ... (kvůli délce nekomentuji; zůstává vše co jsi poslal posledně) ...
# !!! zachovej ve své kopii vše, co jsme minule doplnili (triple barrier, costs, _build_l3_windows atd.) !!!

# ---- jen doplňky níže ----

def set_seed(s: int):
    random.seed(s); np.random.seed(s); torch.manual_seed(s); torch.cuda.manual_seed_all(s)

def export_torchscript(model: nn.Module, example_input: torch.Tensor, out_path: str):
    try:
        model.eval()
        traced = torch.jit.trace(model, example_input)
        torch.jit.save(traced, out_path)
        print(f"[L3] TorchScript export: {out_path}")
    except Exception as e:
        print(f"[L3] TorchScript export failed: {e}")

# (ponechán tvůj train_hmr_supervisor)

def main():
    ap = argparse.ArgumentParser(description="Train L3 Supervisor (HRMHead)")
    # ---- tvoje původní argumenty (ponechány) ----
    # ...
    ap.add_argument("--seeds", type=int, default=1, help="Počet náhodných seedů (bagging).")
    ap.add_argument("--export-torchscript", action="store_true", help="Uložit i TorchScript verzi.")
    args = ap.parse_args()

    if not args.out_dir:
        raise SystemExit("Udej --out_dir (alias --target-set).")
    out_dir = os.path.abspath(args.out_dir)
    os.makedirs(out_dir, exist_ok=True)

    # ---- (přesná kopie tvého čtení meta/sets/printů) ----
    # ...
    # vytvoř X_all, y_all, idx_end, w_all  (stejně jako v tvé verzi)
    # ...
    # split tr/val  (stejně jako v tvé verzi)
    # ...

    in_features = X_all.shape[2]
    device = "cuda" if torch.cuda.is_available() else "cpu"
    num_hsel = len(sets_l2) if args.use_hsel_head else 0

    NSEEDS = max(1, int(args.seeds))
    preds_va = []

    best_ckpts = []
    for si in range(NSEEDS):
        seed = 42 + si*97
        set_seed(seed)
        print(f"[L3] ===== Seed {seed} / {si+1}/{NSEEDS} =====")

        mdl, best = train_hmr_supervisor(
            X_tr, y_tr, w_tr, X_va, y_va, w_va,
            in_features=in_features, seq_len=int(args.l3_seq_len),
            hidden_low=int(args.hidden_low), hidden_high=int(args.hidden_high), high_period=int(args.high_period),
            use_abstain_head=True,
            use_softmax3=bool(args.triple_barrier),
            use_uncert_head=bool(args.use_uncert_head),
            num_hsel=int(num_hsel),
            epochs=int(args.epochs), batch_size=int(args.batch_size), lr=float(args.lr),
            device=device,
            class_weight_up=float(args.class_weight_up), class_weight_down=float(args.class_weight_down)
        )

        mdl.eval()
        with torch.no_grad():
            p_va_up = mdl(torch.from_numpy(X_va).to(device)).reshape(-1).detach().cpu().numpy()
        preds_va.append(p_va_up)

        # uložit seedový checkpoint
        ckpt_path = os.path.join(out_dir, f"supervisor_L3_seed{si}.pt")
        torch.save({
            "state_dict": {k: v.cpu() for k, v in mdl.state_dict().items()},
            "in_features": in_features,
            "hidden_low": int(args.hidden_low),
            "hidden_high": int(args.hidden_high),
            "high_period": int(args.high_period),
            "seq_len": int(args.l3_seq_len),
            "use_abstain_head": True,
            "use_softmax3": bool(args.triple_barrier),
            "use_uncert_head": bool(args.use_uncert_head),
            "num_hsel": int(num_hsel),
        }, ckpt_path)
        best_ckpts.append(os.path.basename(ckpt_path))
        print(f"[L3] uložen: {ckpt_path}")

        if args.export_torchscript:
            ex = torch.from_numpy(X_va[:1]).to(device)
            export_torchscript(mdl, ex, os.path.join(out_dir, f"supervisor_L3_seed{si}.ts"))

    # bagging = průměr pravděpodobností na validaci
    Pva = np.mean(np.stack(preds_va, axis=0), axis=0)
    def _acc_updown(p_up, y):
        m = y[:,1] > 0.5
        if m.sum() == 0: return np.nan
        pred = (p_up[m] >= 0.5).astype(int)
        return (pred == y[m,0].astype(int)).mean()
    print(f"[L3] bagging valid up/down acc={_acc_updown(Pva, y_va):.4f}")

    # Platt kalibrace na bagging výstupu
    calib = None
    if args.platt_l3:
        try:
            from sklearn.linear_model import LogisticRegression
            logit_va = np.log(np.clip(Pva,1e-6,1-1e-6)/(1-np.clip(Pva,1e-6,1-1e-6))).reshape(-1,1)
            m = y_va[:,1] > 0.5
            lr = LogisticRegression(max_iter=1000).fit(logit_va[m], y_va[m,0].astype(int))
            calib = lr
            joblib.dump(calib, os.path.join(out_dir, "supervisor_L3.calib.pkl"))
            print("[L3] Platt kalibrace uložena: supervisor_L3.calib.pkl")
        except Exception as e:
            print(f"[L3] Kalibraci se nepodařilo uložit: {e}")
    else:
        print("[L3] Platt kalibrace vypnuta")

    # uložit seznam seed checkpointů (pro EnsemblePredictor)
    bag_meta = {"seeds": NSEEDS, "files": best_ckpts}
    with open(os.path.join(out_dir, "supervisor_bag.json"), "w") as f:
        json.dump(bag_meta, f, indent=2)
    print(f"[L3] uložen: supervisor_bag.json ({best_ckpts})")

    # supervisor.json + meta thresholds (ponecháno dle tvé verze)
    # ...
    _update_meta_thresholds(out_dir, args.thr_up, args.thr_down, args.thr_margin)

if __name__ == "__main__":
    main()

(Komentáře „…“ znamenají: nech svůj existující kód z minulé verze — nic jsem tam nezkrátil; pouze jsem doplnil seeds a export.)

⸻

7) backtest.py (NOVÝ – jednoduchý backtester se Sharpe)

# app/runtime/backtest.py
# -*- coding: utf-8 -*-
import argparse, json, numpy as np, pandas as pd

def backtest_from_probs(df: pd.DataFrame, thr_up: float, thr_down: float, margin: float,
                        fee_bps: float = 0.0, slip_bps: float = 0.0, horizon: int = 10):
    """
    df: očekává sloupce ['ts', 'close', 'prob_up'] (časově seřazené, 1s krok)
    """
    df = df.sort_values('ts').reset_index(drop=True)
    fee = (fee_bps + slip_bps) / 1e4
    pnl = []
    for i in range(len(df)-horizon):
        p = float(df.loc[i, 'prob_up'])
        c0 = float(df.loc[i, 'close'])
        c1 = float(df.loc[i+horizon, 'close'])
        dec = 0
        if p >= (thr_up + margin): dec = +1
        elif p <= (thr_down - margin): dec = -1
        if dec == 0:
            pnl.append(0.0)
        else:
            r = (c1 - c0) / (c0 + 1e-12)
            g = dec * r - fee
            pnl.append(g)
    pnl = np.array(pnl, dtype=np.float64)
    ret = pnl.mean()
    vol = pnl.std() + 1e-12
    sharpe = ret / vol * np.sqrt(365*24*60*60 / horizon)  # annualizace z Hz
    dd = 0.0
    if len(pnl)>0:
        curve = np.cumsum(pnl)
        peak = np.maximum.accumulate(curve)
        dd = float(np.min(curve - peak))
    return {'mean': float(ret), 'vol': float(vol), 'sharpe': float(sharpe), 'max_dd': dd, 'trades': int((np.abs(pnl)>0).sum())}

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument('--csv', required=True, help='CSV se sloupci ts,close,prob_up (např. z /download/signals)')
    ap.add_argument('--thr-up', type=float, required=True)
    ap.add_argument('--thr-down', type=float, required=True)
    ap.add_argument('--thr-margin', type=float, required=True)
    ap.add_argument('--fee-bps', type=float, default=0.0)
    ap.add_argument('--slip-bps', type=float, default=0.0)
    ap.add_argument('--horizon', type=int, default=10)
    args = ap.parse_args()

    df = pd.read_csv(args.csv)
    res = backtest_from_probs(df, args.thr_up, args.thr_down, args.thr_margin,
                              fee_bps=args.fee_bps, slip_bps=args.slip_bps, horizon=args.horizon)
    print(json.dumps(res, indent=2))

if __name__ == '__main__':
    main()


⸻

8) tune_thresholds.py (NOVÝ – grid/Optuna-free Sharpe tuner)

# app/runtime/tune_thresholds.py
# -*- coding: utf-8 -*-
import argparse, numpy as np, pandas as pd, json
from backtest import backtest_from_probs

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument('--csv', required=True)
    ap.add_argument('--horizon', type=int, default=10)
    ap.add_argument('--fee-bps', type=float, default=0.0)
    ap.add_argument('--slip-bps', type=float, default=0.0)
    ap.add_argument('--grid', type=int, default=11, help='jemnost mřížky (liché číslo)')
    args = ap.parse_args()

    df = pd.read_csv(args.csv)
    best = None
    grid = int(args.grid)
    ups   = np.linspace(0.52, 0.70, grid)
    downs = np.linspace(0.30, 0.48, grid)
    margins = np.linspace(0.00, 0.05, 7)
    for u in ups:
        for d in downs:
            if d >= u: continue
            for m in margins:
                r = backtest_from_probs(df, u, d, m, fee_bps=args.fee_bps, slip_bps=args.slip_bps, horizon=args.horizon)
                key = {'up':float(u), 'down':float(d), 'margin':float(m)}
                if best is None or r['sharpe'] > best['res']['sharpe']:
                    best = {'thr': key, 'res': r}
    print(json.dumps(best, indent=2))

if __name__ == '__main__':
    main()


⸻

9) drift_monitor.py (NOVÝ – PSI mezi live a baseline)

# app/runtime/drift_monitor.py
# -*- coding: utf-8 -*-
import argparse, json, numpy as np, pandas as pd

def psi(expected: np.ndarray, actual: np.ndarray, bins: int = 10) -> float:
    e_hist, edges = np.histogram(expected, bins=bins)
    a_hist, _ = np.histogram(actual, bins=edges)
    e_perc = e_hist / (e_hist.sum() + 1e-12)
    a_perc = a_hist / (a_hist.sum() + 1e-12)
    e_perc = np.clip(e_perc, 1e-6, 1.0); a_perc = np.clip(a_perc, 1e-6, 1.0)
    return float(np.sum((a_perc - e_perc) * np.log(a_perc / e_perc)))

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument('--baseline-parquet', required=True, help='Parquet s baseline featurami (train okno)')
    ap.add_argument('--live-parquet', required=True, help='Parquet s live featurami (stejné sloupce)')
    ap.add_argument('--features', nargs='*', default=None)
    args = ap.parse_args()

    df_b = pd.read_parquet(args.baseline_parquet)
    df_l = pd.read_parquet(args.live_parquet)

    cols = args.features or list(set(df_b.columns) & set(df_l.columns))
    out = {}
    for c in cols:
        try:
            b = df_b[c].dropna().values; a = df_l[c].dropna().values
            if len(b)>100 and len(a)>100:
                out[c] = psi(b, a, bins=10)
        except Exception:
            pass
    print(json.dumps({'psi': out, 'mean': float(np.mean(list(out.values())) if out else 0.0)}, indent=2))

if __name__ == '__main__':
    main()


⸻

10) Jak to použít – příkazy

Trénink L1 (s PurgedKFold + augmenty)

python -m app.models.train_offline \
  --symbol ETHUSDT --days 14 --resume \
  --horizon 1  --seq-len 30 \
  --epochs 12 --purged-kfold --oof-splits 5 --embargo 10 \
  --aug-mixup --aug-timemask \
  --batch 256 --lr 1e-3 \
  --outdir app/models/weights/1s

# ... obdobně pro 10s, 15s, 30s, 60s, 180s (přizpůsob seq-len/horizon)

Trénink META-gate (L2) s PurgedKFold

python -m app.models.train_meta_gate \
  --target-set app/models/weights/10s \
  --sets app/models/weights/1s app/models/weights/5s app/models/weights/10s app/models/weights/15s app/models/weights/30s app/models/weights/60s app/models/weights/180s \
  --l1-seq-len 60 --l2-seq-len 60 \
  --epochs 12 --batch 256 \
  --use-raw-in-l2 \
  --purged-kfold --oof-splits 5 --embargo 10 \
  --platt-l2

Trénink L3 Supervisor (seeds/bagging + TorchScript)

python -m app.models.train_supervisor \
  --data app/data/train_1s.parquet \
  --target-set app/models/weights/10s \
  --sets app/models/weights/1s app/models/weights/5s app/models/weights/10s app/models/weights/15s app/models/weights/30s app/models/weights/60s app/models/weights/180s \
  --l2-heads app/models/weights/1s app/models/weights/10s app/models/weights/15s app/models/weights/30s app/models/weights/60s app/models/weights/180s \
  --use_raw_in_l3 --use-l1-in-l3 --use-l2-in-l3 \
  --l1-seq-len 60 --l3-seq-len 60 \
  --epochs 12 --batch 256 --lr 8e-4 \
  --hrm-hidden-low 96 --hrm-hidden-high 96 --hrm-high-period 12 \
  --valid_frac 0.20 \
  --platt-l3 \
  --seeds 5 --export-torchscript \
  --thr-up 0.58 --thr-down 0.42 --thr-margin 0.02

Runtime – zapnout vážený konsensus + dynamické prahy + bandit

export CONSENSUS_TAGS="1s,10s,15s"
export CONSENSUS_WEIGHTS="1.0,0.8,0.6"
export DYN_THR_ENABLE=1
export POLICY_BANDIT_ENABLE=1
uvicorn app.main:app --host 0.0.0.0 --port 8000

Backtest & Tuning prahů (Sharpe)

# stáhni CSV ze /download/signals
python -m app.runtime.backtest --csv app/data/signals_YYYYMMDD.csv \
  --thr-up 0.58 --thr-down 0.42 --thr-margin 0.02 --fee-bps 1.0 --slip-bps 1.0 --horizon 10

python -m app.runtime.tune_thresholds --csv app/data/signals_YYYYMMDD.csv \
  --horizon 10 --fee-bps 1.0 --slip-bps 1.0 --grid 13

Drift monitor (PSI)

python -m app.runtime.drift_monitor \
  --baseline-parquet app/data/train_1s.parquet \
  --live-parquet app/data/live_buffer.parquet


⸻

Poznámky a limity
	•	train_meta_gate.py: vložil jsem rámec PKF; nahraď uvnitř svůj tréninkový loop tak, aby používal tr_idx/va_idx (je to 1:1 drop-in).
	•	train_offline.py: je tu minimal LSTM větev s PKF a augmenty. Pokud máš XGB/HRM cesty, nech je – stačí doplnit PKF stejně jako u LSTM části.
	•	EnsemblePredictor: pro bagging L3 (více seedů) je dobré, aby loader uměl načíst supervisor_bag.json a zprůměroval výstup; pokud tvůj EnsemblePredictor zatím čte jen jeden supervisor_L3.pt, můžu dodat malý patch, který načte seznam seedových checkpointů a průměruje logity (řekni).
	•	Online kalibrace: držím „offline Platt“ (už funguje). Jestli chceš „online recalib“ (periodic re-fit na posledním bufferu outcomes), můžu dopsat do main.py jednoduchý akumulátor + sklearn.isotonic. Stačí říct a přidám to.

⸻

Pokud chceš, dodám i patch pro EnsemblePredictor, aby:
	•	poznal supervisor_bag.json a průměroval více L3 seedů,
	•	volitelně používal TorchScript místo PyTorch modulu (rychlejší inference).

Řekni „ano“ a pošlu celý soubor ensemble.py jako drop-in.